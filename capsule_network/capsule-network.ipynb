{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import resources\nimport numpy as np\nimport torch\n\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch.optim as optim\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# random seed (for reproducibility)\nseed = 1\n\n# set random seed for numpy\nnp.random.seed(seed)\n\n# set random seed for pytorch\ntorch.manual_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of subprocesses to use for data loading\nnum_workers = 0\n\n# how many samples per batch to load\nbatch_size = 20\n\n# convert data to Tensors\ntransform = transforms.ToTensor()\n\n# choose the training and test datasets\ntrain_data = datasets.MNIST(\n    root='data', \n    train=True, \n    download=True, \n    transform=transform\n)\n\ntest_data = datasets.MNIST(\n    root='data', \n    train=False, \n    download=True, \n    transform=transform\n)\n\n# prepare data loaders\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain one batch of training images\n\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimages = images.numpy()\n\n# plot the images in the batch, along with the corresponding labels\n\nfig = plt.figure(figsize=(25, 4))\n\nfor idx in np.arange(batch_size):\n    \n    ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n    # print out the correct label for each image\n    # .item() gets the value contained in a Tensor\n    ax.set_title(str(labels[idx].item()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvLayer(nn.Module):\n    \n    def __init__(self, in_channels=1, out_channels=256):\n        \n        \"\"\"\n        Constructs the ConvLayer with a specified input and output size.\n        \n        in_channels: input depth of an image, default value = 1\n        out_channels: output depth of the convolutional layer, default value = 256\n        \"\"\"\n        \n        super(ConvLayer, self).__init__()\n\n        # defining a convolutional layer of the specified size\n        \n        self.conv = nn.Conv2d(\n            in_channels, \n            out_channels, \n            kernel_size=9, \n            stride=1, \n            padding=0\n        )\n\n    def forward(self, x):\n        \n        \"\"\"\n        Defines the feedforward behavior.\n        \n        x: the input to the layer; an input image\n        return: a relu-activated, convolutional layer \n        \"\"\"\n        \n        # applying a ReLu activation to the outputs of the conv layer\n        features = F.relu(self.conv(x)) # will have dimensions (batch_size, 20, 20, 256)\n        \n        return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PrimaryCaps(nn.Module):\n    \n    def __init__(self, num_capsules=8, in_channels=256, out_channels=32):\n        \n        \"\"\"\n        Constructs a list of convolutional layers to be used in creating capsule output vectors.\n        \n        num_capsules: number of capsules to create\n        in_channels: input depth of features, default value = 256\n        out_channels: output depth of the convolutional layers, default value = 32\n        \"\"\"\n        \n        super(PrimaryCaps, self).__init__()\n\n        # creating a list of convolutional layers for each capsule I want to create\n        # all capsules have a conv layer with the same parameters\n        self.capsules = nn.ModuleList(\n            [\n                nn.Conv2d(\n                    in_channels=in_channels, \n                    out_channels=out_channels, \n                    kernel_size=9, \n                    stride=2, \n                    padding=0) \n                for _ in range(num_capsules)\n            ]\n        )\n    \n    def forward(self, x):\n        \n        \"\"\"\n        Defines the feedforward behavior.\n        \n        x: the input; features from a convolutional layer\n        return: a set of normalized, capsule output vectors\n        \"\"\"\n        \n        # get batch size of inputs\n        batch_size = x.size(0)\n        \n        # reshape convolutional layer outputs to be (batch_size, vector_dim=1152, 1)\n        u = [ capsule(x).view(batch_size, 32 * 6 * 6, 1) for capsule in self.capsules ]\n        \n        # stack up output vectors, u, one for each capsule\n        u = torch.cat(u, dim=-1)\n        \n        # squashing the stack of vectors\n        u_squash = self.squash(u)\n        \n        return u_squash\n    \n    def squash(self, input_tensor):\n        \n        \"\"\"\n        Squashes an input Tensor so it has a magnitude between 0-1.\n        \n        input_tensor: a stack of capsule inputs, s_j\n        return: a stack of normalized, capsule output vectors, v_j\n        \"\"\"\n        \n        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)\n        scale = squared_norm / (1 + squared_norm) # normalization coeff\n        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)\n        \n        return output_tensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def softmax(input_tensor, dim=1):\n    \n    # transpose input\n    transposed_input = input_tensor.transpose(dim, len(input_tensor.size()) - 1)\n    \n    # calculate softmax\n    softmaxed_output = F.softmax(transposed_input.contiguous().view(-1, transposed_input.size(-1)), dim=-1)\n    \n    # un-transpose result\n    return softmaxed_output.view(*transposed_input.size()).transpose(dim, len(input_tensor.size()) - 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dynamic routing\ndef dynamic_routing(b_ij, u_hat, squash, routing_iterations=3):\n    \n    \"\"\"\n    Performs dynamic routing between two capsule layers.\n    \n    b_ij: initial log probabilities that capsule i should be coupled to capsule j\n    u_hat: input, weighted capsule vectors, W u\n    squash: given, normalizing squash function\n    routing_iterations: number of times to update coupling coefficients\n    return: v_j, output capsule vectors\n    \"\"\"\n    \n    # update b_ij, c_ij for number of routing iterations\n    for iteration in range(routing_iterations):\n        \n        # softmax calculation of coupling coefficients, c_ij\n        c_ij = softmax(b_ij, dim=2)\n\n        # calculating total capsule inputs, s_j = sum(c_ij*u_hat)\n        s_j = (c_ij * u_hat).sum(dim=2, keepdim=True)\n\n        # squashing to get a normalized vector output, v_j\n        v_j = squash(s_j)\n\n        # if not on the last iteration, calculate agreement and new b_ij\n        if iteration < routing_iterations - 1:\n            \n            # agreement\n            a_ij = (u_hat * v_j).sum(dim=-1, keepdim=True)\n            \n            # new b_ij\n            b_ij = b_ij + a_ij\n    \n    return v_j # return latest v_j","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DigitCaps(nn.Module):\n    \n    def __init__(self, num_capsules=10, previous_layer_nodes=32*6*6, in_channels=8, out_channels=16):\n        \n        \"\"\"\n        Constructs an initial weight matrix, W, and sets class variables.\n        \n        num_capsules: number of capsules to create\n        previous_layer_nodes: dimension of input capsule vector, default value = 1152\n        in_channels: number of capsules in previous layer, default value = 8\n        out_channels: dimensions of output capsule vector, default value = 16\n        \"\"\"\n        \n        super(DigitCaps, self).__init__()\n\n        # setting class variables\n        self.num_capsules = num_capsules\n        self.previous_layer_nodes = previous_layer_nodes # vector input (dim=1152)\n        self.in_channels = in_channels # previous layer's number of capsules\n\n        # starting out with a randomly initialized weight matrix, W\n        # these will be the weights connecting the PrimaryCaps and DigitCaps layers\n        self.W = nn.Parameter(\n            torch.randn(\n                num_capsules, \n                previous_layer_nodes, \n                in_channels, out_channels\n            )\n        )\n\n    def forward(self, u):\n        \n        \"\"\"\n        Defines the feedforward behavior.\n        \n        u: the input; vectors from the previous PrimaryCaps layer\n        return: a set of normalized, capsule output vectors\n        \"\"\"\n        \n        # adding batch_size dims and stacking all u vectors\n        u = u[None, :, :, None, :]\n        \n        # 4D weight matrix\n        W = self.W[:, None, :, :, :]\n        \n        # calculating u_hat = W*u\n        u_hat = torch.matmul(u, W)\n\n        # getting the correct size of b_ij\n        # setting them all to 0, initially\n        b_ij = torch.zeros(*u_hat.size())\n        \n        # moving b_ij to GPU, if available\n        b_ij = b_ij.to(device)\n\n        # update coupling coefficients and calculate v_j\n        v_j = dynamic_routing(b_ij, u_hat, self.squash, routing_iterations=3)\n\n        return v_j # return final vector outputs\n    \n    \n    def squash(self, input_tensor):\n        \n        \"\"\"\n        Squashes an input Tensor so it has a magnitude between 0-1.\n        \n        input_tensor: a stack of capsule inputs, s_j\n        return: a stack of normalized, capsule output vectors, v_j\n        \"\"\"\n        \n        # same squash function as before\n        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)\n        scale = squared_norm / (1 + squared_norm) # normalization coeff\n        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)\n        \n        return output_tensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(nn.Module):\n    \n    def __init__(self, input_vector_length=16, input_capsules=10, hidden_dim=512):\n        \n        \"\"\"\n        Constructs an series of linear layers + activations.\n        \n        input_vector_length: dimension of input capsule vector, default value = 16\n        input_capsules: number of capsules in previous layer, default value = 10\n        hidden_dim: dimensions of hidden layers, default value = 512\n        \"\"\"\n        \n        super(Decoder, self).__init__()\n        \n        # calculate input_dim\n        input_dim = input_vector_length * input_capsules\n        \n        # define linear layers + activations\n        self.linear_layers = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim), # first hidden layer\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, hidden_dim*2), # second, twice as deep\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim*2, 28*28), # can be reshaped into 28*28 image\n            nn.Sigmoid() # sigmoid activation to get output pixel values in a range from 0-1\n        )\n        \n    def forward(self, x):\n        \n        \"\"\"\n        Defines the feedforward behavior.\n        \n        x: the input; vectors from the previous DigitCaps layer\n        return: two things, reconstructed images and the class scores, y\n        \"\"\"\n        \n        classes = (x ** 2).sum(dim=-1) ** 0.5\n        classes = F.softmax(classes, dim=-1)\n        \n        # find the capsule with the maximum vector length\n        # here, vector length indicates the probability of a class' existence\n        _, max_length_indices = classes.max(dim=1)\n        \n        # create a sparse class matrix\n        sparse_matrix = torch.eye(10) # 10 is the number of classes\n        sparse_matrix = sparse_matrix.to(device)\n        \n        # get the class scores from the \"correct\" capsule\n        y = sparse_matrix.index_select(dim=0, index=max_length_indices.data)\n        \n        # create reconstructed pixels\n        x = x * y[:, :, None]\n        \n        # flatten image into a vector shape (batch_size, vector_dim)\n        flattened_x = x.contiguous().view(x.size(0), -1)\n        \n        # create reconstructed image vectors\n        reconstructions = self.linear_layers(flattened_x)\n        \n        # return reconstructions and the class scores, y\n        return reconstructions, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CapsuleNetwork(nn.Module):\n    \n    def __init__(self):\n        \n        \"\"\"\n        Constructs a complete Capsule Network.\n        \"\"\"\n        \n        super(CapsuleNetwork, self).__init__()\n        \n        self.conv_layer = ConvLayer()\n        self.primary_capsules = PrimaryCaps()\n        self.digit_capsules = DigitCaps()\n        self.decoder = Decoder()\n                \n    def forward(self, images):\n        \n        \"\"\"\n        Defines the feedforward behavior.\n        \n        images: the original MNIST image input data\n        return: output of DigitCaps layer, reconstructed images, class scores\n        \"\"\"\n        \n        primary_caps_output = self.primary_capsules(self.conv_layer(images))\n        caps_output = self.digit_capsules(primary_caps_output).squeeze().transpose(0,1)\n        reconstructions, y = self.decoder(caps_output)\n        \n        return caps_output, reconstructions, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate and print net\ncapsule_net = CapsuleNetwork()\n\nprint(device)\ncapsule_net.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CapsuleLoss(nn.Module):\n    \n    def __init__(self):\n        \n        \"\"\"\n        Constructs a CapsuleLoss module.\n        \"\"\"\n        \n        super(CapsuleLoss, self).__init__()\n        \n        self.reconstruction_loss = nn.MSELoss(reduction='sum') # cumulative loss, equiv to size_average=False\n\n    def forward(self, x, labels, images, reconstructions):\n        \n        \"\"\"\n        Defines how the loss compares inputs.\n        x: digit capsule outputs\n        labels: \n        images: the original MNIST image input data\n        reconstructions: reconstructed MNIST image data\n        return: weighted margin and reconstruction loss, averaged over a batch\n        \"\"\"\n        \n        batch_size = x.size(0)\n\n        ##  calculate the margin loss   ##\n        \n        # get magnitude of digit capsule vectors, v_c\n        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n\n        # calculate \"correct\" and incorrect loss\n        left = F.relu(0.9 - v_c).view(batch_size, -1)\n        right = F.relu(v_c - 0.1).view(batch_size, -1)\n        \n        # sum the losses, with a lambda = 0.5\n        margin_loss = labels * left + 0.5 * (1. - labels) * right\n        margin_loss = margin_loss.sum()\n\n        ##  calculate the reconstruction loss   ##\n        images = images.view(reconstructions.size()[0], -1)\n        reconstruction_loss = self.reconstruction_loss(reconstructions, images)\n\n        # return a weighted, summed loss, averaged over a batch size\n        return (margin_loss + 0.0005 * reconstruction_loss) / images.size(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# custom loss\ncriterion = CapsuleLoss()\n\n# Adam optimizer with default params\noptimizer = optim.Adam(capsule_net.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(capsule_net, criterion, optimizer, n_epochs, print_every=300):\n    \n    \"\"\"\n    Trains a capsule network and prints out training batch loss statistics.\n    Saves model parameters if *validation* loss has decreased.\n    \n    capsule_net: trained capsule network\n    criterion: capsule loss function\n    optimizer: optimizer for updating network weights\n    n_epochs: number of epochs to train for\n    print_every: batches to print and save training loss, default = 100\n    return: list of recorded training losses\n    \"\"\"\n\n    # track training loss over time\n    losses = []\n\n    # one epoch = one pass over all training data \n    for epoch in range(1, n_epochs+1):\n\n        # initialize training loss\n        train_loss = 0.0\n        \n        capsule_net.train() # set to train mode\n    \n        # get batches of training image data and targets\n        for batch_i, (images, target) in enumerate(train_loader):\n\n            # reshape and get target class\n            target = torch.eye(10).index_select(dim=0, index=target)\n\n            images, target = images.to(device), target.to(device)\n\n            # zero out gradients\n            optimizer.zero_grad()\n            \n            # get model outputs\n            caps_output, reconstructions, y = capsule_net(images)\n            \n            # calculate loss\n            loss = criterion(caps_output, target, images, reconstructions)\n            \n            # perform backpropagation and optimization\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() # accumulated training loss\n            \n            # print and record training stats\n            if batch_i != 0 and batch_i % print_every == 0:\n                \n                avg_train_loss = train_loss/print_every\n                losses.append(avg_train_loss)\n                \n                print('Epoch: {} \\tTraining Loss: {:.8f}'.format(epoch, avg_train_loss))\n                train_loss = 0 # reset accumulated training loss\n        \n    return losses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training for 3 epochs\nn_epochs = 3\nlosses = train(capsule_net, criterion, optimizer, n_epochs=n_epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(losses)\nplt.title(\"Training Loss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(capsule_net, test_loader):\n    \n    \"\"\"\n    Prints out test statistics for a given capsule net.\n    capsule_net: trained capsule network\n    test_loader: test dataloader\n    return: returns last batch of test image data and corresponding reconstructions\n    \"\"\"\n    \n    class_correct = list(0. for i in range(10))\n    class_total = list(0. for i in range(10))\n    \n    test_loss = 0 # loss tracking\n\n    capsule_net.eval() # eval mode\n\n    for batch_i, (images, target) in enumerate(test_loader):\n        \n        target = torch.eye(10).index_select(dim=0, index=target)\n\n        batch_size = images.size(0)\n        \n        images, target = images.to(device), target.to(device)\n\n        # forward pass: compute predicted outputs by passing inputs to the model\n        caps_output, reconstructions, y = capsule_net(images)\n        \n        # calculate the loss\n        loss = criterion(caps_output, target, images, reconstructions)\n        \n        # update average test loss \n        test_loss += loss.item()\n        \n        # convert output probabilities to predicted class\n        _, pred = torch.max(y.data.cpu(), 1)\n        _, target_shape = torch.max(target.data.cpu(), 1)\n\n        # compare predictions to true label\n        correct = np.squeeze(pred.eq(target_shape.data.view_as(pred)))\n        \n        # calculate test accuracy for each object class\n        for i in range(batch_size):\n            \n            label = target_shape.data[i]\n            class_correct[label] += correct[i].item()\n            class_total[label] += 1\n\n    # avg test loss\n    avg_test_loss = test_loss/len(test_loader)\n    \n    print('Test Loss: {:.8f}\\n'.format(avg_test_loss))\n\n    for i in range(10):\n        if class_total[i] > 0:\n            \n            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (str(i), 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])))\n            \n        else:\n            \n            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n\n    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total)))\n    \n    # return last batch of capsule vectors, images, reconstructions\n    return caps_output, images, reconstructions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call test function and get reconstructed images\ncaps_output, images, reconstructions = test(capsule_net, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_images(images, reconstructions):\n    \n    \"\"\"\n    Plot one row of original MNIST images and another row (below) of their reconstructions.\n    \"\"\"\n    \n    # convert to numpy images\n    \n    images = images.data.cpu().numpy()\n    reconstructions = reconstructions.view(-1, 1, 28, 28)\n    reconstructions = reconstructions.data.cpu().numpy()\n    \n    # plot the first ten input images and then reconstructed images\n    \n    fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(26,5))\n\n    # input images on top row, reconstructions on bottom\n    \n    for images, row in zip([images, reconstructions], axes):\n        \n        for img, ax in zip(images, row):\n            \n            ax.imshow(np.squeeze(img), cmap='gray')\n            ax.get_xaxis().set_visible(False)\n            ax.get_yaxis().set_visible(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display original and reconstructed images, in rows\ndisplay_images(images, reconstructions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}